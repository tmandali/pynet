import polars as pl
import os
import math
import json
import time
import logging
from datetime import datetime

class ParquetSynchronizer:
    def __init__(self, db_uri, out_dir, chunk=1_000_000):
        """
        SQL Server ile Parquet dosyalarÄ± arasÄ±nda senkronizasyon saÄŸlar.
        
        Args:
            db_uri (str): 'mssql://user:pass@host/db' formatÄ±nda baÄŸlantÄ± adresi.
            out_dir (str): Parquet dosyalarÄ±nÄ±n tutulacaÄŸÄ± ana dizin.
            chunk (int): ID bazlÄ± dosya bÃ¶lÃ¼mleme boyutu (VarsayÄ±lan: 1 Milyon).
        """
        self.db_uri = db_uri
        self.out_dir = out_dir
        self.chunk = chunk
        
        # Ana klasÃ¶r yoksa oluÅŸtur
        os.makedirs(self.out_dir, exist_ok=True)
        
        # Logger yapÄ±landÄ±rmasÄ±
        self.logger = logging.getLogger(self.__class__.__name__)
        if not self.logger.handlers:
            # Console Handler
            console_handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)
            
            # File Handler
            log_file_path = os.path.join(self.out_dir, "sync_manager.log")
            file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)
            
            self.logger.setLevel(logging.INFO)

    def _bucket(self, pk_val):
        """PK deÄŸerine gÃ¶re dosya numarasÄ±nÄ± (bucket id) hesaplar."""
        if pk_val is None: return 0
        return math.floor(pk_val / self.chunk)

    def _to_hex(self, bytes_val):
        """Binary veriyi SQL sorgusu iÃ§in Hex stringe (0x...) Ã§evirir."""
        if bytes_val is None:
            return "0x0000000000000000"
        return "0x" + bytes_val.hex()

    def _cols_str(self, cols):
        """Kolon listesini SQL formatÄ±na Ã§evirir."""
        if not cols:
            return "*"
        return ", ".join(cols)

    def _paths(self, table):
        """Tabloya Ã¶zel klasÃ¶r ve state dosyasÄ± yollarÄ±nÄ± dÃ¶ner."""
        folder_path = os.path.join(self.out_dir, table)
        state_path = os.path.join(self.out_dir, f"{table}_state.json")
        os.makedirs(folder_path, exist_ok=True)
        return folder_path, state_path

    def _read_state(self, state_path, default_val="0x0000000000000000"):
        """En son baÅŸarÄ±lÄ± iÅŸlenen RowVersion'Ä± JSON dosyasÄ±ndan okur."""
        if os.path.exists(state_path):
            try:
                with open(state_path, "r") as f:
                    data = json.load(f)
                    return data.get("last_rowversion", default_val)
            except Exception:
                return default_val
        return default_val

    def _save_state(self, state_path, last_rv_hex):
        """Ä°ÅŸlem hatasÄ±z biterse son RowVersion'Ä± kaydeder."""
        with open(state_path, "w") as f:
            json.dump({
                "last_rowversion": last_rv_hex,
                "last_update": datetime.now().isoformat()
            }, f)

    def init(self, table, pk, cols=None):
        """
        [INITIALIZE] Tabloyu baÅŸtan sona okur ve ID aralÄ±klarÄ±na gÃ¶re dosyalar oluÅŸturur.
        """
        folder_path, state_path = self._paths(table)
        select_clause = self._cols_str(cols)
        
        self.logger.info(f"\nğŸš€ [INIT] '{table}' baÅŸlatÄ±lÄ±yor...")
        self.logger.info(f"ğŸ“‚ Hedef: {folder_path}")

        # 1. Min/Max ID Bul
        try:
            q_bounds = f"SELECT MIN({pk}) as min_id, MAX({pk}) as max_id FROM {table}"
            bounds_df = pl.read_database_uri(q_bounds, self.db_uri)
            min_id = bounds_df["min_id"][0]
            max_id = bounds_df["max_id"][0]
        except Exception as e:
            self.logger.error(f"âŒ Hata: Tablo sÄ±nÄ±rlarÄ± okunamadÄ±. {e}")
            return

        if min_id is None:
            self.logger.warning("âš ï¸ Tablo boÅŸ.")
            return

        self.logger.info(f"â„¹ï¸ ID AralÄ±ÄŸÄ±: {min_id} - {max_id}")

        # 2. Chunk DÃ¶ngÃ¼sÃ¼
        current_start = min_id
        while current_start <= max_id:
            current_end = current_start + self.chunk
            
            query = f"""
            SELECT {select_clause} FROM {table} 
            WHERE {pk} >= {current_start} AND {pk} < {current_end}
            """
            
            try:
                df_chunk = pl.read_database_uri(query, self.db_uri)
                
                if not df_chunk.is_empty():
                    bucket_id = self._bucket(current_start)
                    file_name = f"part_{bucket_id}.parquet"
                    file_path = os.path.join(folder_path, file_name)
                    
                    df_chunk.write_parquet(file_path)
                    self.logger.info(f"  âœ… YazÄ±ldÄ±: {file_name} ({len(df_chunk)} satÄ±r)")
            except Exception as e:
                self.logger.error(f"  âŒ Hata ({current_start}-{current_end}): {e}")

            current_start = current_end
            
        self.logger.info(f"ğŸ [INIT] '{table}' tamamlandÄ±. Åimdi Upsert Ã§alÄ±ÅŸtÄ±rarak State oluÅŸturabilirsiniz.\n")

    def sync(self, table, pk, ver, cols=None, use_ts=False):
        """
        [INCREMENTAL UPSERT]
        DeÄŸiÅŸen verileri Ã§eker, ilgili dosyalara daÄŸÄ±tÄ±r ve gÃ¼venli ÅŸekilde gÃ¼nceller.
        Checkpoint dosyasÄ± sayesinde iÅŸlem yarÄ±m kalsa bile veri kaybÄ± olmaz.
        """
        folder_path, state_path = self._paths(table)
        select_clause = self._cols_str(cols)
        
        self.logger.info(f"\nğŸ”„ [UPSERT] '{table}' senkronizasyonu baÅŸlÄ±yor...")

        # 1. Checkpoint Oku
        default_rv = "1900-01-01 00:00:00" if use_ts else "0x0000000000000000"
        last_rv_raw = self._read_state(state_path, default_val=default_rv)
        self.logger.info(f"ğŸ“ Son Checkpoint: {last_rv_raw}")

        # SQL Sorgusu iÃ§in deÄŸer hazÄ±rlama
        if use_ts:
            # Datetime string ise SQL'de tÄ±rnak iÃ§inde olmalÄ±
            sql_cmp_value = f"'{last_rv_raw}'"
        else:
            # Binary hex string (0x...) tÄ±rnaksÄ±z kullanÄ±lÄ±r
            sql_cmp_value = last_rv_raw

        # 2. Delta Veriyi Ã‡ek
        # RowVersion binary sÄ±ralamasÄ±na gÃ¼veniyoruz.
        sql_query = f"""
        SELECT {select_clause} FROM {table} 
        WHERE {ver} > {sql_cmp_value}
        ORDER BY {ver} ASC
        """
        
        start_time = time.time()
        try:
            df_new = pl.read_database_uri(sql_query, self.db_uri)
        except Exception as e:
            self.logger.error(f"âŒ SQL BaÄŸlantÄ± HatasÄ±: {e}")
            return

        if df_new.is_empty():
            self.logger.info("âœ… GÃ¼ncel veri yok. Sistem senkronize.")
            return

        # Bu batch iÃ§indeki en bÃ¼yÃ¼k RowVersion'Ä± al (Ä°ÅŸlem biterse bunu kaydedeceÄŸiz)
        max_rv_item = df_new.select(pl.col(ver).max()).item()
        
        if use_ts:
            # Datetime objesini stringe Ã§evir
            max_rv_to_save = str(max_rv_item)
        else:
            # Binary objesini hex stringe Ã§evir
            max_rv_to_save = self._to_hex(max_rv_item)
        
        self.logger.info(f"ğŸ“¥ {len(df_new)} kayÄ±t Ã§ekildi. ({time.time() - start_time:.2f} sn)")

        # 3. Bucket DaÄŸÄ±tÄ±mÄ± ve Upsert
        # Veriyi bucket_id'ye gÃ¶re sanal olarak bÃ¶l
        df_new = df_new.with_columns(
            (pl.col(pk) // self.chunk).alias("bucket_id")
        )
        partitions = df_new.partition_by("bucket_id", as_dict=True)

        try:
            for bucket_val, df_updates in partitions.items():
                file_name = f"part_{bucket_val}.parquet"
                file_path = os.path.join(folder_path, file_name)
                
                # Dosyaya yazarken bucket_id gerekmez
                df_updates_clean = df_updates.drop("bucket_id")

                if os.path.exists(file_path):
                    # --- DOSYA GÃœNCELLEME (Merge) ---
                    df_current = pl.read_parquet(file_path)
                    
                    # Ã–nce mevcut, sonra yeni veriyi ekle
                    df_combined = pl.concat([df_current, df_updates_clean])
                    
                    # ID'ye gÃ¶re tekilleÅŸtir -> Son geleni (gÃ¼ncel olanÄ±) tut
                    df_final = df_combined.unique(subset=[pk], keep="last", maintain_order=False)
                    
                    df_final.write_parquet(file_path)
                    self.logger.info(f"  âœï¸  GÃ¼ncellendi: {file_name} (Toplam: {len(df_final)})")
                else:
                    # --- YENÄ° DOSYA ---
                    df_updates_clean.write_parquet(file_path)
                    self.logger.info(f"  âœ¨ Yeni Dosya: {file_name}")

            # 4. CHECKPOINT KAYDI (Transaction Commit gibi dÃ¼ÅŸÃ¼nÃ¼n)
            # DÃ¶ngÃ¼ hatasÄ±z biterse burasÄ± Ã§alÄ±ÅŸÄ±r.
            self._save_state(state_path, max_rv_to_save)
            self.logger.info(f"ğŸ’¾ Checkpoint gÃ¼ncellendi: {max_rv_to_save}")
            
        except Exception as e:
            self.logger.critical(f"âŒ KRÄ°TÄ°K HATA: Dosya yazma sÄ±rasÄ±nda sorun oluÅŸtu: {e}")
            self.logger.warning("âš ï¸ Checkpoint GÃœNCELLENMEDÄ°. Bir sonraki Ã§alÄ±ÅŸmada veriler tekrar Ã§ekilip dÃ¼zeltilecek.")

        self.logger.info(f"ğŸ [UPSERT] Bitti. SÃ¼re: {time.time() - start_time:.2f} sn\n")


# ==========================================
# Ã‡ALIÅTIRMA BLOÄU (Ã–rnek)
# ==========================================
if __name__ == "__main__":
    
    # 1. BaÄŸlantÄ± AyarlarÄ±
    # LÃ¼tfen kendi sunucu bilgilerinizi girin:
    # Active Directory ile
    CONN_STR = "mssql://testoltp/Store7?driver=ODBC+Driver+17+for+SQL+Server&TrustServerCertificate=yes&trusted_connection=true"

    manager = ParquetSynchronizer(
        db_uri=CONN_STR, 
        out_dir="veri_ambari",  # Parquetlerin duracaÄŸÄ± klasÃ¶r
        chunk=1_000_000               # 1 Milyonluk dilimler
    )

    # --- A. Ä°LK YÃœKLEME (Sadece 1 kere Ã§alÄ±ÅŸtÄ±rÄ±n) ---
    manager.init(
       table="tb_UrunRecete",
        pk="ID"
    )

    # --- B. ARTIMLI GÃœNCELLEME (Cron/Schedule ile Ã§alÄ±ÅŸtÄ±rÄ±n) ---
    #manager.sync(
    #    table="tb_UrunRecete",
    #    pk="SatisId",
    #    ver="RowVersion",
    #    cols=["SatisId", "UrunAdi", "Tutar", "RowVersion"]
    #)

    # --- C. DATETIME ILE ARTIMLI GÃœNCELLEME Ã–RNEÄÄ° ---
    # manager.sync(
    #     table="Loglar",
    #     pk="LogId",
    #     ver="CreatedDate",
    #     cols=["LogId", "Message", "CreatedDate"],
    #     use_ts=True
    # )